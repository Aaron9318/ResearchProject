{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Resnet34.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMSa8DxzLAiB4a3oH79+Q2v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VsZw1hz_pW5p"},"source":["## Imports\n"]},{"cell_type":"code","metadata":{"id":"jMd1TKaa77CO"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Exez9A33AFRB"},"source":["import time\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","#from torchvision import datasets\n","#from torchvision import transforms\n","import glob, os\n","from tqdm import tqdm\n","import torchvision\n","#State the main folder to read the files is located here\n","#os.chdir(r\"/content/drive/MyDrive/ResearchProject/Data\")\n","os.chdir(r\"C:\\Users\\demon\\Documents\\Maestria y Papeles Alemania\\ResearchProject\\datos\\Data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQ6oz2l_1vqK"},"source":["from collections import defaultdict\n","from functools import partial\n","from multiprocessing import cpu_count\n","from pathlib import Path\n","from textwrap import dedent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZhiHome13Bn"},"source":["#import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import math\n","import joblib\n","#import sklearn.externals \n","#from sklearn.externals import joblib\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.nn import functional as F\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.utils.data import TensorDataset, DataLoader\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifKfOz9bfnUk"},"source":["# From local helper files\n","from helper_evaluation import set_all_seeds, set_deterministic, compute_confusion_matrix\n","from helper_train import train_model\n","from helper_plotting import plot_training_loss, plot_accuracy, show_examples, plot_confusion_matrix\n","from helper_dataset import get_dataloaders_cifar10, UnNormalize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQb_ztc-UHf8"},"source":["## DatasetReading"]},{"cell_type":"code","metadata":{"id":"IrbaoL6zm8QJ"},"source":["import h5py\n","\n","with h5py.File(r\"dataBaselinecut1000.h5\", \"r\") as f:\n","#with h5py.File(r\"dataBaselinecut1000.h5\", \"r\") as f:\n","    # List all groups\n","    print(\"Keys: %s\" % f.keys())\n","    a_group_key = list(f.keys())[0]\n","\n","    # Get the data\n","    data = list(f[a_group_key])\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RMuNrOmpEmE"},"source":["with h5py.File(r\"binomialclass.h5\", \"r\") as f:\n","    # List all groups\n","    print(\"Keys: %s\" % f.keys())\n","    a_group_key = list(f.keys())[0]\n","\n","    # Get the data\n","    wayu = list(f[a_group_key])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oS6QQ8onjcA"},"source":["raw_arr = np.array(data)\n","target = np.array(wayu)\n","print(raw_arr.shape)\n","print(target.shape)\n","random_seed = 123\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXCFXUr9SU8q"},"source":["values, counts =  np.unique(target, return_counts=True)\n","itemindex = np.where(target == 0)\n","itemindex1 = np.where(target == 1)\n","\n","valmin =  min(counts)\n","valmax =  max(counts)\n","dif =  valmax-valmin\n","b =  np.random.choice(itemindex1[0],dif,replace=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UaTByrqyAnR"},"source":["raw_even = raw_arr\n","alfa = np.delete(raw_even, b, axis=0)\n","beta = np.delete(target,b, axis = 0)\n","#print(raw_even.shape)\n","#print(alfa.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2cs_ABh8PPr"},"source":["print(beta.shape)\n","print(alfa.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2X-qPjQqzgt"},"source":["#raw_arr =  raw_arr.transpose(0,2,1)\n","#print(raw_arr.shape)\n","alfa = alfa.transpose(0,2,1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqSRT-g-GM6P"},"source":["\n","print(beta.shape)\n","print(alfa.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6t7jqXW717cp"},"source":["seed = 1 \n","np.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yNYLQDHCpNa"},"source":["def create_datasets(data, target, train_size, valid_pct=0.1, seed=None):\n","    \"\"\"Converts NumPy arrays into PyTorch datsets.\n","    \n","    Three datasets are created in total:\n","        * training dataset\n","        * validation dataset\n","        * testing (un-labelled) dataset\n","\n","    \"\"\"\n","    #raw, fft = data\n","    #assert len(raw) == len(fft)\n","    raw = data\n","    sz = train_size\n","    idx = np.arange(sz)\n","    trn_idx, val_idx = train_test_split(\n","        idx, test_size=valid_pct, random_state=seed)\n","    trn_ds = TensorDataset(\n","        torch.tensor(raw[:sz][trn_idx]).float(), \n","        #torch.tensor(fft[:sz][trn_idx]).float(), \n","        torch.tensor(target[:sz][trn_idx]).long())\n","    val_ds = TensorDataset(\n","        torch.tensor(raw[:sz][val_idx]).float(), \n","        #torch.tensor(fft[:sz][val_idx]).float(), \n","        torch.tensor(target[:sz][val_idx]).long())\n","    tst_ds = TensorDataset(\n","        torch.tensor(raw[sz:]).float(), \n","        #torch.tensor(fft[sz:]).float(), \n","        torch.tensor(target[sz:]).long())\n","    return trn_ds, val_ds, tst_ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4EHu4QdKCskU"},"source":["def create_loaders(data, bs=128, jobs=0):\n","    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n","    \n","    trn_ds, val_ds, tst_ds = data\n","    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n","    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n","    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n","    return trn_dl, val_dl, tst_dl\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hEIc_czaloR"},"source":["raw_feat = alfa.shape[1]\n","print(raw_feat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyHE5i3Xg60o"},"source":["trn_sz = round(len(alfa)*.7)\n"," # only the first `trn_sz` rows in each array include labelled data\n","datasets = create_datasets(alfa,beta, trn_sz, seed=seed)\n","trn_da, val_da, tst_da = datasets\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaaXGU0jnlwI"},"source":["#raw_feat = raw_arr.shape[2]\n","#fft_feat = fft_arr.shape[1]\n","\n","trn_dl, val_dl, tst_dl = create_loaders(datasets,bs=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqe5BGnIZZ06"},"source":["torch.cuda.is_available()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USZyeknBTqUc"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"Bo-JGxuYQWjH"},"source":["##########################\n","### SETTINGS\n","##########################\n","\n","RANDOM_SEED = 123\n","BATCH_SIZE = 256\n","NUM_EPOCHS = 50\n","DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URYNaGzJQYXG"},"source":["set_all_seeds(RANDOM_SEED)\n","#set_deterministic()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYx8-483rHKy"},"source":["\n","def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return torch.nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return torch.nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(torch.nn.Module):\n","    expansion: int = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None,\n","                 groups=1, base_width=64, dilation=1, norm_layer=None):\n","        \n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = torch.nn.BatchNorm1d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(torch.nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion=4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None,\n","                 groups=1, base_width=64, dilation=1, norm_layer=None):\n","        \n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = torch.nn.BatchNorm1d\n","        width = int(planes * (base_width / 64.)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(torch.nn.Module):\n","\n","    def __init__(self, block, layers, num_classes, zero_init_residual=False, groups=1,\n","                 width_per_group=64, replace_stride_with_dilation=None, norm_layer=None):\n","        \n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = torch.nn.BatchNorm1d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\"replace_stride_with_dilation should be None \"\n","                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = torch.nn.Conv1d(11, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                                     bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.maxpool = torch.nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n","                                       dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n","                                       dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n","                                       dilate=replace_stride_with_dilation[2])\n","        self.avgpool = torch.nn.AdaptiveAvgPool1d(1)\n","        self.fc = torch.nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, torch.nn.Conv1d):\n","                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, (torch.nn.BatchNorm1d, torch.nn.GroupNorm)):\n","                torch.nn.init.constant_(m.weight, 1)\n","                torch.nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    torch.nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    torch.nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks,\n","                    stride=1, dilate=False):\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = torch.nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n","                            self.base_width, previous_dilation, norm_layer))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, groups=self.groups,\n","                                base_width=self.base_width, dilation=self.dilation,\n","                                norm_layer=norm_layer))\n","\n","        return torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVMUJfbUO1ty"},"source":["model = ResNet(BasicBlock, layers=[3, 4, 6, 3], num_classes=2) # ResNet34\n","\n","model = model.to(device)\n","\n","optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                       factor=0.1,\n","                                                       mode='max',\n","                                                       verbose=True)\n","\n","minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n","    model=model,\n","    num_epochs=NUM_EPOCHS,\n","    train_loader=trn_dl,\n","    valid_loader=val_dl,\n","    test_loader=tst_dl,\n","    optimizer=optimizer,\n","    device=device,\n","    scheduler=scheduler,\n","    scheduler_on='valid_acc',\n","    logging_interval=100)\n","\n","plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n","                   num_epochs=NUM_EPOCHS,\n","                   iter_per_epoch=len(trnd_dl),\n","                   results_dir=None,\n","                   averaging_iterations=200)\n","plt.show()\n","\n","plot_accuracy(train_acc_list=train_acc_list,\n","              valid_acc_list=valid_acc_list,\n","              results_dir=None)\n","plt.ylim([60, 100])\n","plt.show()"],"execution_count":null,"outputs":[]}]}